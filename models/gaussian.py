import math

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

import models
from models import register


def generate_meshgrid(height, width):
    """generate meshgrid with shape [height*width, 2]"""
    # Generate all pixel coordinates for the given image dimensions
    y_coords, x_coords = torch.arange(0, height), torch.arange(0, width)
    # Create a grid of coordinates
    yy, xx = torch.meshgrid(y_coords, x_coords)
    # Flatten and stack the coordinates to obtain a list of (x, y) pairs
    all_coords = torch.stack([xx.flatten(), yy.flatten()], dim=1)

    return all_coords


def fetching_features_from_tensor(image_tensor, input_coords):
    """Querying the value at the specified coordinates of the tensor"""
    # Assumption: image_tensor is of shape [batch, channel, height, width]
    #             input_coords is of shape [N, 2], where each row is (x, y)
    batch_size = image_tensor.shape[0]
    # Normalising pixel coordinates to [-1, 1] for grid_sample
    if not isinstance(input_coords, torch.Tensor):
        input_coords = torch.tensor(input_coords, dtype=torch.float32)
    input_coords = input_coords.to(image_tensor.device)
    coords = input_coords.float()
    coords[:, 0] = (coords[:, 0] / (image_tensor.shape[-1] - 1)) * 2.0 - 1.0
    coords[:, 1] = (coords[:, 1] / (image_tensor.shape[-2] - 1)) * 2.0 - 1.0
    # Create a batched grid with Batch and Channel dimensions as 1
    grid = coords.unsqueeze(0).unsqueeze(0)
    grid = grid.expand(batch_size, -1, -1, -1)
    grid = grid.to(image_tensor.device)
    color_values_tensor = torch.nn.functional.grid_sample(
        image_tensor, grid, mode='bilinear', padding_mode='border', align_corners=True)
    # Squeeze to remove the output spatial dimensions, now the shape is [batch, channel, N]
    color_values_tensor = color_values_tensor.squeeze(2).squeeze(2)
    # Transpose the dimensions to [batch, N, channel]
    color_values_tensor = color_values_tensor.permute(0, 2, 1)

    return color_values_tensor, coords


@register('gaussiansplatter')
class GaussianSplatter(nn.Module):
    def __init__(self, encoder_spec, kernel_size, num_points):

        super(GaussianSplatter, self).__init__()

        self.feat = None
        self.encoder = models.make(encoder_spec)
        self.out_dim = self.encoder.out_dim

        # key parameter in GaussianSplatter
        self.kernel_size = kernel_size
        self.root = round(math.sqrt(num_points))
        if num_points != self.root * self.root:
            raise ValueError("num_points must be a square number")
        self.num_points = num_points
        self.sigma_x = None  # std in x-axis
        self.sigma_y = None  # std in y-axis
        self.coords = None   # coord of LR, shape=[num_points, 2], each element in batch share the same coords
        self.colors = None   # color(feature) of LR, shape=[batch, num_points, channel of LR feature]
        self.alpha = None    # transparency of color(feature), shape=[num_points, 1], each element in batch share the same coords

    def gen_feat(self, inp):
        self.feat = self.encoder(inp)  # self.feat.shape = [Batch, 64, 48, 48]
        return self.feat

    def LR_initialization(self):
        # Initialized using LR image
        if self.feat is None:
            raise ValueError("must be applied after def gen_feat")

        length = self.feat.shape[-2] * self.feat.shape[-1]
        sigma_values = 0.5 * torch.ones(length, 2)
        self.sigma_x, self.sigma_y = sigma_values[:, 0], sigma_values[:, 1]
        self.alpha = torch.ones(length, 1)

        # 4 trainable parameters: colors(feature), sigma_x, sigma_y, alpha
        # self.colors = nn.Parameter(self.colors)
        self.sigma_x = nn.Parameter(self.sigma_x)
        self.sigma_y = nn.Parameter(self.sigma_y)
        self.alpha = nn.Parameter(self.alpha)

    def forward(self, inp):
        """
            inp: expect [Batch, C, H, W] tensor, generated by encoder
            sigma_x, sigma_y: expect [N] tensor,
            coords: expect [N, 2] tensor,
            colors: expect [Batch, N, 3] tensor.
        """
        device = inp.device

        self.gen_feat(inp)
        image_size = self.feat.shape
        if self.coords is None:
            self.LR_initialization()
        coords_ = generate_meshgrid(self.root, self.root)
        self.colors, self.coords = fetching_features_from_tensor(self.feat, coords_)

        if image_size[0] != self.colors.shape[0] or image_size[1] != self.colors.shape[-1]:
            raise ValueError("Batch/Channel size of colors does not match the image size.")
        batch_size, channel, _, _ = image_size
        gaussian_point_num = self.colors.shape[1]
        sigma_x = self.sigma_x.view(gaussian_point_num, 1, 1)
        sigma_y = self.sigma_y.view(gaussian_point_num, 1, 1)
        covariance = torch.stack(
            [torch.stack([sigma_x ** 2, torch.zeros(sigma_x.shape)], dim=-1),
             torch.stack([torch.zeros(sigma_x.shape), sigma_y ** 2], dim=-1)], dim=-2
        )  # when correlation rou is set to zero, covariance will always be positive semi-definite

        inv_covariance = torch.inverse(covariance).to(device)

        # Choosing quite a broad range for the distribution [-5,5] to avoid any clipping
        start = torch.tensor([-5.0], device=device).view(-1, 1)
        end = torch.tensor([5.0], device=device).view(-1, 1)
        base_linspace = torch.linspace(0, 1, steps=self.kernel_size, device=device)
        ax_batch = start + (end - start) * base_linspace

        # Expanding dims for broadcasting
        ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, self.kernel_size)
        ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, self.kernel_size, -1)

        # Creating a batch-wise meshgrid using broadcasting
        xx, yy = ax_batch_expanded_x, ax_batch_expanded_y

        xy = torch.stack([xx, yy], dim=-1)
        z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)
        kernel = torch.exp(z) / (
                2 * torch.tensor(np.pi, device=device) * torch.sqrt(torch.det(covariance)).to(device).view(gaussian_point_num, 1, 1))

        kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)  # Find max along the last dimension
        kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)  # Find max along the second-to-last dimension
        kernel_normalized = kernel / kernel_max_2

        kernel_reshaped = kernel_normalized.repeat(1, channel, 1).view(gaussian_point_num * channel, self.kernel_size,
                                                                       self.kernel_size)
        kernel_color = kernel_reshaped.unsqueeze(0).reshape(gaussian_point_num, channel, self.kernel_size, self.kernel_size)

        # Calculating the padding needed to match the image size
        pad_h = image_size[-2] - self.kernel_size
        pad_w = image_size[-1] - self.kernel_size

        if pad_h < 0 or pad_w < 0:
            raise ValueError("Kernel size should be smaller or equal to the image size.")

        # Adding padding to make kernel size equal to the image size
        padding = (pad_w // 2, pad_w // 2 + pad_w % 2,
                   pad_h // 2, pad_h // 2 + pad_h % 2)

        kernel_color_padded = torch.nn.functional.pad(kernel_color, padding, "constant", 0)

        # Extracting shape information
        b, c, h, w = kernel_color_padded.shape

        # Create a batch of 2D affine matrices
        theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)
        theta[:, 0, 0] = 1.0
        theta[:, 1, 1] = 1.0
        theta[:, :, 2] = self.coords

        # Creating grid and performing grid sampling
        grid = F.affine_grid(theta, size=[b, c, h, w], align_corners=True)
        kernel_color_padded_translated = F.grid_sample(kernel_color_padded, grid, align_corners=True)
        kernel_color_padded_translated = kernel_color_padded_translated.unsqueeze(0).expand(batch_size, -1, -1, -1, -1)
        colors = self.colors * self.alpha.to(device).unsqueeze(0).expand(batch_size, -1, -1)
        color_values_reshaped = colors.unsqueeze(-1).unsqueeze(-1)

        final_image_layers = color_values_reshaped * kernel_color_padded_translated
        final_image = final_image_layers.sum(dim=1)
        final_image = torch.clamp(final_image, 0, 1)

        return final_image
