import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

import models
from models import register


def generate_meshgrid(height, width):
    """generate meshgrid (type: torch.tensor) with shape [height * width, 2]"""
    # Generate all pixel coordinates for the given image dimensions
    y_coords, x_coords = torch.arange(0, height), torch.arange(0, width)
    # Create a grid of coordinates
    yy, xx = torch.meshgrid(y_coords, x_coords)
    # Flatten and stack the coordinates to obtain a list of (x, y) pairs
    all_coords = torch.stack([xx.flatten(), yy.flatten()], dim=1)

    return all_coords


def fetching_features_from_tensor(image_tensor, input_coords):
    """Querying the value at the specified coordinates of the tensor"""
    # Assumption: image_tensor is of shape [batch, channel, height, width]
    #             input_coords is of shape [N, 2], where each row is (x, y)
    # normalising pixel coordinates [-1,1]
    input_coords = input_coords.to(image_tensor.device)
    coords = input_coords / torch.tensor([image_tensor.shape[-2], image_tensor.shape[-1]],
                                         device=image_tensor.device).float()
    center_coords_normalized = torch.tensor([0.5, 0.5], device=image_tensor.device).float()
    coords = (center_coords_normalized - coords) * 2.0

    # Fetching the colour of the pixels in each coordinates
    batch_size = image_tensor.shape[0]
    input_coords_expanded = input_coords.unsqueeze(0).expand(batch_size, -1, -1)

    y_coords = input_coords_expanded[..., 0].long()
    x_coords = input_coords_expanded[..., 1].long()
    batch_indices = torch.arange(batch_size).view(-1, 1).to(input_coords.device)

    color_values = image_tensor[batch_indices, :, x_coords, y_coords]

    return color_values, coords


@register('gaussiansplatter')
class GaussianSplatter:
    def __init__(self, encoder_spec, kernel_size, num_row_points=48, num_column_points=48):
        super(GaussianSplatter, self).__init__()

        self.feat = None
        self.encoder = models.make(encoder_spec)
        self.out_dim = self.encoder.out_dim

        # key parameter in GaussianSplatter
        self.kernel_size = kernel_size
        self.row = num_row_points
        self.column = num_column_points  # Initialize a gaussian point grid
        self.num_points = num_column_points * num_row_points

        self.sigma_x = None  # std in x-axis
        self.sigma_y = None  # std in y-axis
        self.coords = None   # coord of LR, shape=[num_points, 2], each element in batch share the same coords
        self.opacity = None  # transparency of feature, shape=[num_points, 1], each element in batch share the same opacity

    def gen_feat(self, inp):
        self.feat = self.encoder(inp)  # self.feat.shape = [Batch, Channel, Height, Width]
        return self.feat

    def initialization(self):
        # Initialization
        sigma_values = 0.5 * torch.ones(self.row * self.column, 2)
        self.sigma_x, self.sigma_y = sigma_values[:, 0], sigma_values[:, 1]
        self.opacity = torch.ones(self.row * self.column, 1)

        # 3 trainable parameters: sigma_x, sigma_y, opacity
        self.sigma_x = nn.Parameter(self.sigma_x)
        self.sigma_y = nn.Parameter(self.sigma_y)
        self.opacity = nn.Parameter(self.opacity)

    def expand_features(self):
        """Replicates and rearranges feature attributes of 2d gaussian grid to fit the input spatial domain
        at arbitrary scales and maintaining the original pattern by indexing using modular arithmetic."""
        new_row, new_column = self.feat.shape[-2], self.feat.shape[-1]

        # Generate expanded row/column indices for the new domain by wrapping around the original number of rows/columns
        expanded_row_indices = torch.arange(new_row) % self.row
        expanded_column_indices = torch.arange(new_column) % self.column

        # Compute a 2D grid of indices that map the new domain back to the original domain
        expanded_indices = expanded_row_indices.unsqueeze(1) * self.column + expanded_column_indices
        expanded_indices_flat = expanded_indices.view(-1)

        expanded_sigma_x = self.sigma_x[expanded_indices_flat]
        expanded_sigma_y = self.sigma_y[expanded_indices_flat]
        expanded_opacity = self.opacity[expanded_indices_flat]

        return expanded_sigma_x, expanded_sigma_y, expanded_opacity

    def forward(self, inp):
        """
            inp: expect [Batch, C, H, W] tensor, generated by encoder
            sigma_x, sigma_y: expect [N] tensor,
            coords: expect [N, 2] tensor,
            colors: expect [Batch, N, 3] tensor.
        """
        device = inp.device

        self.gen_feat(inp)
        image_size = self.feat.shape
        if self.coords is None:
            self.initialization()
        coords_ = generate_meshgrid(self.feat.shape[-2], self.feat.shape[-1])
        num_feature_points = self.feat.shape[-2] * self.feat.shape[-1]
        colors_, self.coords = fetching_features_from_tensor(self.feat, coords_)

        batch_size, channel, _, _ = image_size
        # Spread Gaussian points over the whole feature map
        expanded_sigma_x, expanded_sigma_y, expanded_opacity = self.expand_features()

        sigma_x = expanded_sigma_x.view(num_feature_points, 1, 1)
        sigma_y = expanded_sigma_y.view(num_feature_points, 1, 1)
        covariance = torch.stack(
            [torch.stack([sigma_x ** 2, torch.zeros(sigma_x.shape)], dim=-1),
             torch.stack([torch.zeros(sigma_x.shape), sigma_y ** 2], dim=-1)], dim=-2
        )  # when correlation rou is set to zero, covariance will always be positive semi-definite

        inv_covariance = torch.inverse(covariance).to(device)

        # Choosing quite a broad range for the distribution [-5,5] to avoid any clipping
        start = torch.tensor([-5.0], device=device).view(-1, 1)
        end = torch.tensor([5.0], device=device).view(-1, 1)
        base_linspace = torch.linspace(0, 1, steps=self.kernel_size, device=device)
        ax_batch = start + (end - start) * base_linspace

        # Expanding dims for broadcasting
        ax_batch_expanded_x = ax_batch.unsqueeze(-1).expand(-1, -1, self.kernel_size)
        ax_batch_expanded_y = ax_batch.unsqueeze(1).expand(-1, self.kernel_size, -1)

        # Creating a batch-wise meshgrid using broadcasting
        xx, yy = ax_batch_expanded_x, ax_batch_expanded_y

        xy = torch.stack([xx, yy], dim=-1)
        z = torch.einsum('b...i,b...ij,b...j->b...', xy, -0.5 * inv_covariance, xy)
        kernel = torch.exp(z) / (
                2 * torch.tensor(np.pi, device=device) * torch.sqrt(torch.det(covariance)).to(device).view(num_feature_points, 1, 1))

        kernel_max_1, _ = kernel.max(dim=-1, keepdim=True)  # Find max along the last dimension
        kernel_max_2, _ = kernel_max_1.max(dim=-2, keepdim=True)  # Find max along the second-to-last dimension
        kernel_normalized = kernel / kernel_max_2

        kernel_reshaped = kernel_normalized.repeat(1, channel, 1).view(num_feature_points * channel, self.kernel_size,
                                                                       self.kernel_size)
        kernel_color = kernel_reshaped.unsqueeze(0).reshape(num_feature_points, channel, self.kernel_size, self.kernel_size)

        # Calculating the padding needed to match the image size
        pad_h = image_size[-2] - self.kernel_size
        pad_w = image_size[-1] - self.kernel_size

        if pad_h < 0 or pad_w < 0:
            raise ValueError("Kernel size should be smaller or equal to the image size.")

        # Adding padding to make kernel size equal to the image size
        padding = (pad_w // 2, pad_w // 2 + pad_w % 2,
                   pad_h // 2, pad_h // 2 + pad_h % 2)

        kernel_color_padded = torch.nn.functional.pad(kernel_color, padding, "constant", 0)

        # Extracting shape information
        b, c, h, w = kernel_color_padded.shape

        # Create a batch of 2D affine matrices
        theta = torch.zeros(b, 2, 3, dtype=torch.float32, device=device)
        theta[:, 0, 0] = 1.0
        theta[:, 1, 1] = 1.0
        theta[:, :, 2] = self.coords

        # Creating grid and performing grid sampling
        grid = F.affine_grid(theta, size=[b, c, h, w], align_corners=True)
        kernel_color_padded_translated = F.grid_sample(kernel_color_padded, grid, align_corners=True)
        kernel_color_padded_translated = kernel_color_padded_translated.unsqueeze(0).expand(batch_size, -1, -1, -1, -1)
        colors = colors_ * expanded_opacity.to(device).unsqueeze(0).expand(batch_size, -1, -1)
        color_values_reshaped = colors.unsqueeze(-1).unsqueeze(-1)

        final_image_layers = color_values_reshaped * kernel_color_padded_translated
        final_image = final_image_layers.sum(dim=1)
        final_image = torch.clamp(final_image, 0, 1)

        return final_image
